{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4533800,"sourceType":"datasetVersion","datasetId":2649037}],"dockerImageVersionId":30715,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Naive Bayes Classification for Sentiment Analysis on IMDB Movie Reviews","metadata":{}},{"cell_type":"markdown","source":"**Description:**\n\nIn this notebook, we explore the power of Naive Bayes for sentiment analysis, a fundamental task in natural language processing (NLP). We'll dive into the world of movie reviews, using the popular IMDB dataset to train and evaluate two variations of the Naive Bayes classifier:\n\n* Multinomial Naive Bayes: This model is well-suited for text classification where word frequency matters. We'll see how it performs in distinguishing positive and negative movie reviews based on the occurrence of words.\n* Bernoulli Naive Bayes: This model focuses on the presence or absence of words, making it a good choice when the mere existence of certain words is indicative of sentiment. We'll compare its performance to the Multinomial variant.","metadata":{}},{"cell_type":"markdown","source":"**Key Highlights:**\n\n* Data Preprocessing: We'll walk through essential steps to clean and prepare the text data, including tokenization, stop word removal, and handling special characters. Understanding these steps is crucial for any NLP project.\n* Feature Extraction (Vectorization): We'll demonstrate how to convert raw text into numerical representations that Naive Bayes can understand. We'll use techniques like CountVectorizer to create feature vectors based on word frequencies.\n* Model Training and Evaluation: We'll train both Multinomial and Bernoulli Naive Bayes models on the IMDB dataset and evaluate their performance using metrics like accuracy, precision, recall, and F1-score. We'll discuss the strengths and weaknesses of each model.\n* Comparison and Insights: We'll analyze the results, comparing the two Naive Bayes variations and drawing insights into their suitability for sentiment analysis tasks.\n","metadata":{}},{"cell_type":"markdown","source":"**Who Should Read This:**\n\nThis notebook is perfect for:\n\n* Beginners in NLP: If you're new to natural language processing, this is a great starting point to understand fundamental concepts and build a practical classifier.\n* Anyone interested in Sentiment Analysis: Whether you're a data scientist, marketer, or simply curious about how machines understand emotions in text, this notebook offers a hands-on approach to sentiment analysis.\n* Fans of Naive Bayes: If you want to see how this simple yet effective algorithm can be applied to real-world text data, this is the notebook for you.","metadata":{}},{"cell_type":"markdown","source":"**Let's get started and uncover the sentiment hidden within movie reviews!**\n\n****","metadata":{}},{"cell_type":"markdown","source":"**Import the required library**","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-07-28T07:25:17.169497Z","iopub.execute_input":"2024-07-28T07:25:17.170018Z","iopub.status.idle":"2024-07-28T07:25:17.602452Z","shell.execute_reply.started":"2024-07-28T07:25:17.169973Z","shell.execute_reply":"2024-07-28T07:25:17.601327Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing data","metadata":{}},{"cell_type":"markdown","source":"**1. Reading dataset**","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/imdb-review/aclImdb'","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:25:17.604290Z","iopub.execute_input":"2024-07-28T07:25:17.604768Z","iopub.status.idle":"2024-07-28T07:25:17.609229Z","shell.execute_reply.started":"2024-07-28T07:25:17.604737Z","shell.execute_reply":"2024-07-28T07:25:17.608193Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"folders = [\n    ('train', 'neg', 'negative'),\n    ('train', 'pos', 'positive'),\n    ('test', 'neg', 'negative'),\n    ('test', 'pos', 'positive')\n]\ndata = []","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:25:17.610662Z","iopub.execute_input":"2024-07-28T07:25:17.610996Z","iopub.status.idle":"2024-07-28T07:25:17.620096Z","shell.execute_reply.started":"2024-07-28T07:25:17.610968Z","shell.execute_reply":"2024-07-28T07:25:17.618634Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for split, sentiment, label in folders:\n    folder_path = os.path.join(data_dir, split, sentiment)\n    for filename in os.listdir(folder_path):\n        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            data.append({ 'split': split, 'label': label,'text': text})\ndf = pd.DataFrame(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:25:17.623467Z","iopub.execute_input":"2024-07-28T07:25:17.624023Z","iopub.status.idle":"2024-07-28T07:28:09.728358Z","shell.execute_reply.started":"2024-07-28T07:25:17.623979Z","shell.execute_reply":"2024-07-28T07:28:09.727141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Data infor","metadata":{}},{"cell_type":"code","source":"print(df.tail())","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:09.729904Z","iopub.execute_input":"2024-07-28T07:28:09.730326Z","iopub.status.idle":"2024-07-28T07:28:09.747295Z","shell.execute_reply.started":"2024-07-28T07:28:09.730288Z","shell.execute_reply":"2024-07-28T07:28:09.746055Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"      split     label                                               text\n49995  test  positive  typically, a movie can have factors like \"arou...\n49996  test  positive  Like the first film in this series (SLAUGHTER,...\n49997  test  positive  The problem with so many people watching this ...\n49998  test  positive  Not a bad MOW. I was expecting another film ba...\n49999  test  positive  Despite myself, I really kinda like this movie...\n","output_type":"stream"}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:09.748739Z","iopub.execute_input":"2024-07-28T07:28:09.749429Z","iopub.status.idle":"2024-07-28T07:28:09.789344Z","shell.execute_reply.started":"2024-07-28T07:28:09.749388Z","shell.execute_reply":"2024-07-28T07:28:09.788090Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   split   50000 non-null  object\n 1   label   50000 non-null  object\n 2   text    50000 non-null  object\ndtypes: object(3)\nmemory usage: 1.1+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Here are some samples of negative reviews in the test kit**","metadata":{}},{"cell_type":"code","source":"test_negative_reviews = df[(df['split'] == 'test') & (df['label'] == 'negative')]","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:09.790791Z","iopub.execute_input":"2024-07-28T07:28:09.791134Z","iopub.status.idle":"2024-07-28T07:28:09.814812Z","shell.execute_reply.started":"2024-07-28T07:28:09.791102Z","shell.execute_reply":"2024-07-28T07:28:09.813316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test_negative_reviews.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:09.816370Z","iopub.execute_input":"2024-07-28T07:28:09.816747Z","iopub.status.idle":"2024-07-28T07:28:09.831170Z","shell.execute_reply.started":"2024-07-28T07:28:09.816715Z","shell.execute_reply":"2024-07-28T07:28:09.829968Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      split     label                                               text\n25000  test  negative  Committed doom and gloomer Peter Watkins goes ...\n25001  test  negative  Most critics have written devastating about th...\n25002  test  negative  Did I waste my time. This is very pretentious ...\n25003  test  negative  What a stinker!!! I swear this movie was writt...\n25004  test  negative  Ever had one of those nights when you couldn't...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>split</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25000</th>\n      <td>test</td>\n      <td>negative</td>\n      <td>Committed doom and gloomer Peter Watkins goes ...</td>\n    </tr>\n    <tr>\n      <th>25001</th>\n      <td>test</td>\n      <td>negative</td>\n      <td>Most critics have written devastating about th...</td>\n    </tr>\n    <tr>\n      <th>25002</th>\n      <td>test</td>\n      <td>negative</td>\n      <td>Did I waste my time. This is very pretentious ...</td>\n    </tr>\n    <tr>\n      <th>25003</th>\n      <td>test</td>\n      <td>negative</td>\n      <td>What a stinker!!! I swear this movie was writt...</td>\n    </tr>\n    <tr>\n      <th>25004</th>\n      <td>test</td>\n      <td>negative</td>\n      <td>Ever had one of those nights when you couldn't...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":" **2. Separate training and test sets:**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df['text']\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:09.832444Z","iopub.execute_input":"2024-07-28T07:28:09.832788Z","iopub.status.idle":"2024-07-28T07:28:10.540901Z","shell.execute_reply.started":"2024-07-28T07:28:09.832758Z","shell.execute_reply":"2024-07-28T07:28:10.539728Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":" 20% of the data will be selected as a test set and the remaining 80% will be  a training set.\n ran_state = 1 will return the same dataset when random the next time.\n ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'text':X_train,'Label':y_train}).head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:10.545048Z","iopub.execute_input":"2024-07-28T07:28:10.545514Z","iopub.status.idle":"2024-07-28T07:28:10.563925Z","shell.execute_reply.started":"2024-07-28T07:28:10.545474Z","shell.execute_reply":"2024-07-28T07:28:10.562661Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                    text     Label\n18165  Roman Polanski plays Trelkovsky who rents an a...  positive\n36059  I saw this movie last night at the Berlinale a...  negative\n13242  This program is a lot of fun and the title son...  positive\n32985  I love the first and third Beastmasters, but t...  negative\n41133  Ghoulies IV may not be the best out of the ser...  positive\n9273   In the small American town of Meadowvale Dr. A...  negative\n12784  this movie probably had a $750 budget, and sti...  positive\n43992  Great just great! The West Coast got \"Dirty\" H...  positive\n33452  When I first saw this movie, I said to myself,...  negative\n6342   Masters of Horror: The Screwfly Solution start...  negative","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18165</th>\n      <td>Roman Polanski plays Trelkovsky who rents an a...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>36059</th>\n      <td>I saw this movie last night at the Berlinale a...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>13242</th>\n      <td>This program is a lot of fun and the title son...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>32985</th>\n      <td>I love the first and third Beastmasters, but t...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>41133</th>\n      <td>Ghoulies IV may not be the best out of the ser...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>9273</th>\n      <td>In the small American town of Meadowvale Dr. A...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12784</th>\n      <td>this movie probably had a $750 budget, and sti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>43992</th>\n      <td>Great just great! The West Coast got \"Dirty\" H...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>33452</th>\n      <td>When I first saw this movie, I said to myself,...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>6342</th>\n      <td>Masters of Horror: The Screwfly Solution start...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**3. Transform text into a characteristic matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX_train_counts = vectorizer.fit_transform(X_train)\nX_test_counts = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:10.565394Z","iopub.execute_input":"2024-07-28T07:28:10.566049Z","iopub.status.idle":"2024-07-28T07:28:20.637045Z","shell.execute_reply.started":"2024-07-28T07:28:10.566010Z","shell.execute_reply":"2024-07-28T07:28:20.635713Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**How's it work ?**","metadata":{}},{"cell_type":"markdown","source":"**+ fit_tranform**","metadata":{}},{"cell_type":"markdown","source":"CountVectorizer will turn all the words in the given set into 1 dictionary, where keys are words and values are the number of occurrences of that word in the set.\n**Ex if the doct is\"Hello world, this is a beautiful world\"\nthen the dict will be\n{\"hello\":1,\"world\":2,\"this\" : 1....}**","metadata":{}},{"cell_type":"markdown","source":"this is the first 10 element of dict:","metadata":{}},{"cell_type":"code","source":"cnt = 0\nfor word,i in vectorizer.vocabulary_.items():\n    print(f'{word} : {i}')\n    cnt +=1\n    if(cnt==10): break","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.638382Z","iopub.execute_input":"2024-07-28T07:28:20.638733Z","iopub.status.idle":"2024-07-28T07:28:20.645263Z","shell.execute_reply.started":"2024-07-28T07:28:20.638704Z","shell.execute_reply":"2024-07-28T07:28:20.644104Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"roman : 69798\npolanski : 62985\nplays : 62640\ntrelkovsky : 84294\nwho : 90343\nrents : 68110\nan : 3970\napartment : 4711\nin : 40845\nfrance : 31869\n","output_type":"stream"}]},{"cell_type":"markdown","source":"+ **tranform**\n","metadata":{}},{"cell_type":"markdown","source":"Create a vector whose size coincides with the frequency vector (dictionary), bearing the value 1 where it appears in the dictionary, and 0 if it does not appear","metadata":{}},{"cell_type":"code","source":"new_sentence = [\"in an apartment\"]  # Đặt chuỗi vào danh sách\nnew_sentence_vector = vectorizer.transform(new_sentence).toarray()\nprint(new_sentence_vector)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.646844Z","iopub.execute_input":"2024-07-28T07:28:20.647174Z","iopub.status.idle":"2024-07-28T07:28:20.661370Z","shell.execute_reply.started":"2024-07-28T07:28:20.647147Z","shell.execute_reply":"2024-07-28T07:28:20.660031Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It returns a sparse matrix where the elements corresponding to the words in the new sentence are 1, and the rest are 0. These are the non-zero elements in the vector.","metadata":{}},{"cell_type":"code","source":"nonZero = new_sentence_vector[new_sentence_vector !=0 ]\nprint(nonZero)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.662819Z","iopub.execute_input":"2024-07-28T07:28:20.663287Z","iopub.status.idle":"2024-07-28T07:28:20.674274Z","shell.execute_reply.started":"2024-07-28T07:28:20.663233Z","shell.execute_reply":"2024-07-28T07:28:20.672946Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[1 1 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Naive Bayes model training","metadata":{}},{"cell_type":"markdown","source":"\n**In the Naive Bayes algorithm, we need to calculate the probability of a sentence x being positive (p) or negative (n). To determine this, we calculate the probability of sentence x being positive (P(p|x)) or negative (P(n|x)), compare them, and if P(p|x) > P(n|x), then the sentence is positive, otherwise it is negative.**\n","metadata":{}},{"cell_type":"markdown","source":"To calculate the probability P(p|x) (*P(n|x) is calculated similarly*), we use Bayes' theorem:\n\n* P(p|x) = P(x|p) * P(p) / P(x)  or, in general terms:\n     P(a|b) = P(b|a) * P(a) / P(b)\n* Therefore, the goal of the model is to calculate P(x|p), P(p), and P(x).","metadata":{}},{"cell_type":"markdown","source":"# Key Concepts of the Naive Bayes Algorithm in Sentiment Classification (Positive/Negative)","metadata":{}},{"cell_type":"markdown","source":"1. step 1.  Classification: For a given sentence \"x,\" the algorithm calculates the probability that it is positive (p(x|p)) and the probability that it is negative (p(x|n)). By comparing these probabilities, the sentence is classified as positive if p(x|p) > p(x|n), and negative otherwise.\n2. step 2.  Training: During the training phase, the model pre-computes necessary values before encountering the sentence to be classified. To calculate p(x|p) in step 1, Bayes' theorem is applied: P(p|x) = P(x|p) * P(p) / P(x). The prior probability P(p) can be determined beforehand. To streamline the calculation of P(x|p) and P(x) for future sentences, the training phase also calculates the probability of each word in the vocabulary belonging to the positive (p) or negative (n) class. These probabilities are stored in a likelihood dictionary.","metadata":{}},{"cell_type":"markdown","source":"1. Note 1: Likelihood Smoothing: In the likelihood dictionary, some words may not appear in any positive (p) sentences, resulting in a zero probability when calculating their likelihood in the positive class. To prevent this, we add a smoothing factor (typically 1) to all word counts. Since each word receives this +1 smoothing, the total number of smoothing additions equals the vocabulary size. Therefore, the denominator of the probability calculation must also be increased by the vocabulary size.\n\n2. Note 2: Logarithmic Calculation: To calculate the probability of a sentence belonging to class (p) or (n), the standard formula is P(x|p) * P(p) / P(x). The prior probability P(p) is pre-computed and stored in a dictionary. A useful trick is to use the logarithm of the likelihood values (log(likelihood)) to avoid underflow issues when multiplying very small probabilities. Additionally, logarithms transform the complex multiplication of probabilities into a simpler addition of log-probabilities. The constant P(x) is the same for all classes and can be omitted from the comparison. We only need to calculate P(x|p) * P(p) using log addition.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef train_multinomial_nb(X_train, y_train):\n    # After TranFrom, train_test is a 2-dimensional dataframe with rows\n        #as sentences in the TRAIN set, columns as words in the dictionary\n    classes = np.unique(y_train)\n    #Next, we will caculate P(p) (and P(n) when using in negative case)\n    # priors is a dict have keys is nag or pos, values is P(n) and P(p)\n    priors = {c: np.sum(y_train == c) / len(y_train) for c in classes}\n    \n    \n    #Next is p(x|p), which is calculated as the product of p(xi|p) according\n    # to the Bayesian independent variables hypothesis. And in this step\n    # we will calculate the probability of all i words\n    # in the dictionary under the condition p (or n).\n        \n    likelihoods = {}\n    for c in classes:\n        \n        #Ex c = positive\n        #class_docs is a 2-dimensional dataframe \n        #whose rows are positive review sentences encoded into vector, \n        #columns are words in the dictionary\n        class_docs = X_train[y_train == c]\n        \n        #Calculate the total number of words in the positive class,\n        #then divide the total number of times the word i\n        #appears in the positive class\n        #by the total number of words that will have P(xi|p)\n        total_words = class_docs.sum()\n        \n        \n        #There are words that do not appear in the positive class, \n        #so the value = 0 and cause an error. So we +1 on all words \n        #to avoid the value 0 (Laplace smoothing):\n        #        class_docs.sum(axis=0) + 1 \n        \n        #However, the total number of words should also increase these 1s,\n        #and the total number of increases will be the total number of words \n        # in the dictionary:\n        #           total_words + X_train.shape[1]\n        \n        likelihoods[c] = (class_docs.sum(axis=0) + 1) / (total_words + X_train.shape[1])\n    return priors, likelihoods\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.675902Z","iopub.execute_input":"2024-07-28T07:28:20.676331Z","iopub.status.idle":"2024-07-28T07:28:20.687571Z","shell.execute_reply.started":"2024-07-28T07:28:20.676291Z","shell.execute_reply":"2024-07-28T07:28:20.686408Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate model with test set**","metadata":{}},{"cell_type":"code","source":"def evaluate_multinomial_nb(X_test, y_test, priors, likelihoods):\n    y_pred = []\n    for doc in X_test:\n        #To avoid that the probabilities of actions that are too small\n        #to multiply each other will be close to zero,\n        #we calculate their probabilities multiplied by each other\n        \n        #We will have a dictionary consisting of posterior probabilities\n        #of each word in the dictionary, provided that they belong to the\n        #positive or negative class, respectively\n        probs = {c: np.log(priors[c]) for c in priors}\n        \n        \n        for i, count in zip(doc.indices, doc.data):\n            for c in priors:\n                probs[c] += np.log(likelihoods[c][0, i]) * count\n        #The predicted result will be based on max (p(pos) ,p(neg))\n        y_pred.append(max(probs, key=probs.get))\n        \n    correct = np.sum(y_pred == y_test)\n    accuracy = correct / len(y_test)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.689224Z","iopub.execute_input":"2024-07-28T07:28:20.689582Z","iopub.status.idle":"2024-07-28T07:28:20.704180Z","shell.execute_reply.started":"2024-07-28T07:28:20.689543Z","shell.execute_reply":"2024-07-28T07:28:20.703095Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate model with new review**\n","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(review, vectorizer, priors, likelihoods):\n    X_new = vectorizer.transform([review])\n    \n    #Initialization of the logarithmic probability dictionary. \n        #Initially, each class is assigned a logarithmic value of \n        #its priori probability. The use of logarithms helps avoid underflow \n        #when calculating with very small probabilities.\n    probs = {c: np.log(priors[c]) for c in priors}\n    \n    #data: An array containing nonzero values of the matrix.\n    #indices: An array containing column indexes of other values that \n    #do not correspond in data\n    for i, count in zip(X_new.indices, X_new.data):\n        for c in priors:\n            \n            probs[c] += np.log(likelihoods[c][0, i]) * count\n    predicted_class = max(probs, key=probs.get)\n    return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.705740Z","iopub.execute_input":"2024-07-28T07:28:20.706352Z","iopub.status.idle":"2024-07-28T07:28:20.720391Z","shell.execute_reply.started":"2024-07-28T07:28:20.706310Z","shell.execute_reply":"2024-07-28T07:28:20.719335Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\npriors, likelihoods = train_multinomial_nb(X_train_counts, y_train)\naccuracy = evaluate_multinomial_nb(X_test_counts, y_test, priors, likelihoods)\nprint(f\"Accuracy on test set: {accuracy:.2f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:28:20.721767Z","iopub.execute_input":"2024-07-28T07:28:20.722197Z","iopub.status.idle":"2024-07-28T07:28:29.149028Z","shell.execute_reply.started":"2024-07-28T07:28:20.722158Z","shell.execute_reply":"2024-07-28T07:28:29.147770Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy on test set: 0.84\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Testing in new review**","metadata":{}},{"cell_type":"code","source":"# Predict new review, feel free to test your review\nnew_review = '''\"The Grand Illusion\" is a masterpiece of cinematic storytelling. The film's deliberate pacing allows for nuanced character development and the exploration of complex themes. The performances are superb, and the cinematography is visually stunning. The film's emotional impact is profound, leaving a lasting impression on the viewer.'''\npredicted_sentiment = predict_sentiment(new_review, vectorizer, priors, likelihoods)\nprint(f\"Predicted sentiment: {predicted_sentiment}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:35:12.191813Z","iopub.execute_input":"2024-07-28T07:35:12.192198Z","iopub.status.idle":"2024-07-28T07:35:12.199474Z","shell.execute_reply.started":"2024-07-28T07:35:12.192167Z","shell.execute_reply":"2024-07-28T07:35:12.198486Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Predicted sentiment: positive\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict new review, feel free to test your review\nnew_review = '''Overhyped and underwhelming, \"The Grand Illusion\" is a disappointment. The film's dialogue is stilted, and the acting is uninspired. The cinematography is unoriginal, relying on tired tropes and cliches. The film's attempt to tackle complex social issues is superficial and ultimately fails to deliver any meaningful insights.'''\npredicted_sentiment = predict_sentiment(new_review, vectorizer, priors, likelihoods)\nprint(f\"Predicted sentiment: {predicted_sentiment}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T07:35:59.432342Z","iopub.execute_input":"2024-07-28T07:35:59.432714Z","iopub.status.idle":"2024-07-28T07:35:59.439174Z","shell.execute_reply.started":"2024-07-28T07:35:59.432686Z","shell.execute_reply":"2024-07-28T07:35:59.438128Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Predicted sentiment: negative\n","output_type":"stream"}]}]}